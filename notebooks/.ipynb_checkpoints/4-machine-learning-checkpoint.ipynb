{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2000)\n"
     ]
    }
   ],
   "source": [
    "digits = cv2.imread('../datasets/digits.png', 0)\n",
    "print(digits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 100, 20, 20)\n"
     ]
    }
   ],
   "source": [
    "images = [np.hsplit(row, 100) for row in np.vsplit(digits, 50)]\n",
    "\n",
    "images = np.array(images, dtype = np.float32)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "train_features = images[:, :50].reshape(-1, (20 * 20))\n",
    "\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "test_features = images[:, 50:100].reshape(-1, (20 * 20))\n",
    "\n",
    "print(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 1)\n"
     ]
    }
   ],
   "source": [
    "k = np.arange(10)\n",
    "train_labels = np.repeat(k, 250).reshape(-1, 1)\n",
    "test_labels = train_labels.copy()\n",
    "\n",
    "train_features -= np.mean(train_features, axis=0)\n",
    "test_features -= np.mean(train_features, axis=0)\n",
    "\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = cv2.ml.KNearest_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.train(train_features, cv2.ml.ROW_SAMPLE, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, result, neighbors, dist = knn.findNearest(test_features, 3)\n",
    "\n",
    "matches = np.equal(result, test_labels)\n",
    "matches = matches.astype(np.int)\n",
    "\n",
    "correct = np.count_nonzero(matches)\n",
    "\n",
    "accuracy = (correct * 100.00) / result.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.24\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(840, 840)\n"
     ]
    }
   ],
   "source": [
    "fashion = cv2.imread('../datasets/fashion.png', 0)\n",
    "print(fashion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 100, 20, 20)\n"
     ]
    }
   ],
   "source": [
    "images = [np.hsplit(row, 100) for row in np.vsplit(digits, 50)]\n",
    "\n",
    "images = np.array(images, dtype = np.float32)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "train_features = images[:, :50].reshape(-1, (20 * 20))\n",
    "\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "test_features = images[:, 50:100].reshape(-1, (20 * 20))\n",
    "\n",
    "print(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 1)\n"
     ]
    }
   ],
   "source": [
    "k = np.arange(10)\n",
    "train_labels = np.repeat(k, 250).reshape(-1, 1)\n",
    "test_labels = train_labels.copy()\n",
    "\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = cv2.ml.KNearest_create()\n",
    "\n",
    "knn.train(train_features, cv2.ml.ROW_SAMPLE, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.64\n"
     ]
    }
   ],
   "source": [
    "ret, result, neighbors, dist = knn.findNearest(test_features, 3)\n",
    "\n",
    "matches = np.equal(result, test_labels)\n",
    "matches = matches.astype(np.int)\n",
    "\n",
    "correct = np.count_nonzero(matches)\n",
    "\n",
    "accuracy = (correct * 100.00) / result.size\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                        #########ACTIVITY############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(840, 840)\n"
     ]
    }
   ],
   "source": [
    "fashion = cv2.imread('../datasets/fashion.png', 0)\n",
    "print(fashion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 30, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "fashion_images = [np.hsplit(row, 30) for row in np.vsplit(fashion, 30)]\n",
    "\n",
    "fashion_images = np.array(fashion_images, dtype = np.float32)\n",
    "\n",
    "plt.imshow(fashion_images[1,1], cmap = 'gray')\n",
    "plt.show()   \n",
    "\n",
    "print(fashion_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "fashion_train_features = fashion_images[:, :15].reshape(-1, (28 * 28))\n",
    "\n",
    "print(fashion_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "fashion_test_features = fashion_images[:, 15:30].reshape(-1, (28 * 28))\n",
    "\n",
    "print(fashion_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 1)\n"
     ]
    }
   ],
   "source": [
    "fashion_k = np.arange(10)\n",
    "fashion_train_labels = np.repeat(fashion_k, 45).reshape(-1, 1)\n",
    "fashion_test_labels = fashion_train_labels.copy()\n",
    "\n",
    "print(fashion_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.88888888888889\n"
     ]
    }
   ],
   "source": [
    "fashion_knn = cv2.ml.KNearest_create()\n",
    "\n",
    "fashion_knn.train(fashion_train_features, cv2.ml.ROW_SAMPLE, fashion_train_labels)\n",
    "\n",
    "ret, result, neighbors, dist = fashion_knn.findNearest(fashion_test_features, 3)\n",
    "\n",
    "matches = np.equal(result, fashion_test_labels)\n",
    "matches = matches.astype(np.int)\n",
    "\n",
    "correct = np.count_nonzero(matches)\n",
    "\n",
    "accuracy = (correct * 100.00) / result.size\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                    ############DIGITS_SVM#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.44\n"
     ]
    }
   ],
   "source": [
    "digits = cv2.imread('../datasets/digits.png', 0)\n",
    "\n",
    "images = [np.hsplit(row, 100) for row in np.vsplit(digits, 50)]\n",
    "images = np.array(images, dtype=np.float32)\n",
    "\n",
    "model = cv2.ml.SVM_create()\n",
    "\n",
    "model.setKernel(cv2.ml.SVM_LINEAR)\n",
    "model.setC(2.67)\n",
    "model.setGamma(5.383)\n",
    "model.setType(cv2.ml.SVM_C_SVC)\n",
    "\n",
    "model.train(train_features, cv2.ml.ROW_SAMPLE, train_labels)\n",
    "\n",
    "result = model.predict(test_features)\n",
    "\n",
    "matches = np.equal(result[1], test_labels)\n",
    "matches = matches.astype(np.int)\n",
    "\n",
    "correct = np.count_nonzero(matches)\n",
    "accuracy = ((correct * 100.00) / result[1].size)\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                    ############FASHION_SVM#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.1) C:\\projects\\opencv-python\\opencv\\modules\\ml\\src\\svm.cpp:1624: error: (-5) in the case of classification problem the responses must be categorical; either specify varType when creating TrainData, or pass integer responses in function cv::ml::SVMImpl::train\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-083db658c5a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVM_C_SVC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfashion_train_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mROW_SAMPLE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfashion_train_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfashion_test_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(3.4.1) C:\\projects\\opencv-python\\opencv\\modules\\ml\\src\\svm.cpp:1624: error: (-5) in the case of classification problem the responses must be categorical; either specify varType when creating TrainData, or pass integer responses in function cv::ml::SVMImpl::train\n"
     ]
    }
   ],
   "source": [
    "fashion = cv2.imread('../datasets/fashion.png', 0)\n",
    "\n",
    "fashion_images = [np.hsplit(row, 30) for row in np.vsplit(fashion, 30)]\n",
    "fashion_images = np.array(fashion_images, dtype=np.float32)\n",
    "\n",
    "model = cv2.ml.SVM_create()\n",
    "\n",
    "model.setKernel(cv2.ml.SVM_LINEAR)\n",
    "model.setC(2.67)\n",
    "model.setGamma(5.383)\n",
    "model.setType(cv2.ml.SVM_C_SVC)\n",
    "\n",
    "model.train(fashion_train_features, cv2.ml.ROW_SAMPLE, fashion_train_labels)\n",
    "\n",
    "result = model.predict(fashion_test_features)\n",
    "\n",
    "matches = np.equal(result[1], fashion_test_labels)\n",
    "matches = matches.astype(np.int)\n",
    "\n",
    "correct = np.count_nonzero(matches)\n",
    "accuracy = ((correct * 100.00) / result[1].size)\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                            ################Mean Normalization#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
